{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad621fe3-db61-4734-bc6e-c1ab6f6d3068",
   "metadata": {},
   "source": [
    "# Sentiment analysis using Bag-of-Words representation\n",
    "The goal is to develop a model capable of accurately classifying text data into positive and negative sentiment categories. Several steps were made including text preprocessing, feature extraction using CountVectorizer, and model selection. Hyperparameter tuning was performed using grid search with cross-validation to optimise the classifier's performance. The final model was evaluated on both validation and test sets to assess its real-world performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e429fd-aeb3-4fe1-b08e-46ffc0617eb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1: Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf38dee7-ec04-4878-a724-460790f62e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b540061-3b38-4f54-9cab-0a88d4c77b4b",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab319a3a-a083-4695-af75-e4ba9030a2ff",
   "metadata": {},
   "source": [
    "A dataset of several thousand single-sentence reviews collected from three domains: imdb.com,\n",
    "amazon.com, yelp.com. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d185fa-1076-42db-aa3f-caabceb62079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "x_train = pd.read_csv('Dataset/x_train.csv', names=['website_name', 'text'], header=None)\n",
    "y_train = pd.read_csv('Dataset/y_train.csv', names=['is_positive_sentiment'], header=None)\n",
    "# Load test data\n",
    "x_test = pd.read_csv('Dataset/x_test.csv', names=['website_name', 'text'], header=None)\n",
    "y_test = pd.read_csv('Dataset/y_test.csv', names=['is_positive_sentiment'], header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4227e1-6345-4d6b-a63a-054e18709289",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7ca345-f238-4887-9429-9947d615ff35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    text = text.lower() # Convert upper case to lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove multiple consecutive spaces\n",
    "    text = text.strip() # Remove leading and trailing spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd93b03f-9490-48de-beb2-dbacff5ad178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean data\n",
    "x_train['text'] = x_train['text'].apply(clean_text)\n",
    "x_test['text'] = x_test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28799ca9-6bdd-4ef4-b2eb-0c2645a9d90a",
   "metadata": {},
   "source": [
    "### Splitting the training data into a training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9245957d-0626-4df3-b229-3ada6b5bae43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_train['text'], y_train['is_positive_sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cae2ccaf-949c-4e1d-b1ff-bb1396929500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reshape y_train and y_val\n",
    "y_train = y_train.ravel()\n",
    "y_val = y_val.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb1b21-454d-4d5a-bb73-41c40628a807",
   "metadata": {},
   "source": [
    "## Task 2: Feature Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5de4b01-49f9-4718-96c4-7ab579d2c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'SVC': SVC(),\n",
    "    'Logistic Regression': LogisticRegression()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "904c5237-dcf0-4b4f-938f-1fe95cd69a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store mean cross-validation scores for each classifier\n",
    "cv_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e5dfda-951a-464c-a618-7734c743eb7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Random Forest:\n",
      "Best parameters:\n",
      "{'vect__binary': True, 'vect__lowercase': True, 'vect__max_df': 0.5, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None}\n",
      "\n",
      "Testing Decision Tree:\n",
      "Best parameters:\n",
      "{'vect__binary': True, 'vect__lowercase': True, 'vect__max_df': 0.7, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None}\n",
      "\n",
      "Testing SVC:\n",
      "Best parameters:\n",
      "{'vect__binary': True, 'vect__lowercase': True, 'vect__max_df': 0.5, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': None}\n",
      "\n",
      "Testing Logistic Regression:\n",
      "Best parameters:\n",
      "{'vect__binary': True, 'vect__lowercase': True, 'vect__max_df': 0.5, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over classifiers\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"Testing {clf_name}:\")\n",
    "    \n",
    "    # Define the pipeline for each classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    # Define the parameters for grid search\n",
    "    parameters = {\n",
    "        'vect__lowercase': [True, False],\n",
    "        'vect__stop_words': [None, 'english'],\n",
    "        'vect__max_df': [0.5, 0.7, 1.0],\n",
    "        'vect__min_df': [1, 5, 10],\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'vect__binary': [True, False],\n",
    "    }\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1, error_score='raise')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store mean cross-validation score\n",
    "    cv_scores[clf_name] = np.mean(cross_val_score(grid_search.best_estimator_, X_train, y_train, cv=5))\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(\"Best parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6de24f0-437f-461c-ba3f-ae890e1b84d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best classifier is: Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "# Pick the best classifier based on mean cross-validation score\n",
    "best_classifier = max(cv_scores, key=cv_scores.get)\n",
    "print(f\"The best classifier is: {best_classifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c42b5c70-e542-483b-8a70-05cdd75b5fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best best parameters for CountVectorizer is: CountVectorizer(binary=True, max_df=0.5, ngram_range=(1, 2))\n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer with best parameters\n",
    "# Get the CountVectorizer from the pipeline\n",
    "vectorizer = grid_search.best_estimator_.named_steps['vect']\n",
    "print(f\"The best best parameters for CountVectorizer is: {vectorizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4438b00b-d910-4ee8-b144-49a69417f28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer with best parameters\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "X_test = vectorizer.transform(x_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640bdbe-676e-478d-a218-b828c8568664",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3: Classification and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d366606-407b-4432-b802-09de9d2155e4",
   "metadata": {},
   "source": [
    "### Performing the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c830cac7-58e1-4dbe-9e90-8377b1acbde6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.1, 1, 10],\n",
       "                         &#x27;max_iter&#x27;: [800, 1000, 2000, 4000, 10000],\n",
       "                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;lbfgs&#x27;, &#x27;newton-cg&#x27;, &#x27;sag&#x27;,\n",
       "                                    &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;f1&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.1, 1, 10],\n",
       "                         &#x27;max_iter&#x27;: [800, 1000, 2000, 4000, 10000],\n",
       "                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;lbfgs&#x27;, &#x27;newton-cg&#x27;, &#x27;sag&#x27;,\n",
       "                                    &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;f1&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.01, 0.1, 1, 10],\n",
       "                         'max_iter': [800, 1000, 2000, 4000, 10000],\n",
       "                         'solver': ['liblinear', 'lbfgs', 'newton-cg', 'sag',\n",
       "                                    'saga']},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifier Selection and Training\n",
    "model = LogisticRegression()\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "    'max_iter': [800, 1000, 2000, 4000, 10000]\n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='f1', verbose=1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc28fd9-7df3-4381-87e0-4cd5a9e4ae33",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0601e2ae-0495-4689-a62e-df2a975733cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10, max_iter=10000, solver='saga')\n"
     ]
    }
   ],
   "source": [
    "# Best classifier\n",
    "best_classifier = grid_search.best_estimator_\n",
    "print(best_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79569d70-dc82-4d12-82ed-3725283776de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform predictions\n",
    "y_pred_train = best_classifier.predict(X_train) # Training set\n",
    "y_pred_val = best_classifier.predict(X_val) # Validation set\n",
    "y_pred_test = best_classifier.predict(X_test) # Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acf4b365-63a2-48cd-8ac0-68da3a36127a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       963\n",
      "           1       1.00      1.00      1.00       957\n",
      "\n",
      "    accuracy                           1.00      1920\n",
      "   macro avg       1.00      1.00      1.00      1920\n",
      "weighted avg       1.00      1.00      1.00      1920\n",
      "\n",
      "Validation set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       237\n",
      "           1       0.87      0.82      0.84       243\n",
      "\n",
      "    accuracy                           0.85       480\n",
      "   macro avg       0.85      0.85      0.85       480\n",
      "weighted avg       0.85      0.85      0.85       480\n",
      "\n",
      "Test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       300\n",
      "           1       0.84      0.77      0.81       300\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.82      0.81      0.81       600\n",
      "weighted avg       0.82      0.81      0.81       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display classification reports \n",
    "print(\"Training set\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "print(\"Validation set\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "print(\"Test set\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc97151",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics\n",
    "\n",
    "On the training set, the LR classifier achieved a precision of 100% for both negative sentiment\n",
    "reviews (label 0) and positive sentiment reviews (label 1), with corresponding recall scores of 100%.\n",
    "The F1-score, which is the harmonic mean of precision and recall, was 1.00 for both negative and\n",
    "positive sentiment, resulting in an overall accuracy of 100%.\n",
    "\n",
    "Similarly, on the validation set, the classifier demonstrated a precision of 83% for negative sentiment\n",
    "and 87% for positive sentiment, with recall scores of 87% and 82%, respectively. The F1-scores were\n",
    "0.85 for negative sentiment and 0.84 for positive sentiment, yielding an accuracy of 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9bbb80e-8295-44c7-a6aa-83f9d278ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe for test set analysis\n",
    "test_analysis = x_test.copy()\n",
    "test_analysis['label'] = y_test\n",
    "test_analysis['predict'] = y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eec75846-c156-4e05-9a3a-5a8ff78d78a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to analyze sentence length\n",
    "def analyze_sentence_length(df):\n",
    "    \"\"\"\n",
    "    Analyzes the performance of the model based on sentence length.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): DataFrame containing text data.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Tuple containing confusion matrices for short, medium, and long sentences.\n",
    "    \"\"\"\n",
    "    df['sentence_length'] = df['text'].apply(lambda x: len(x.split()))\n",
    "    # Compare performance based on sentence length\n",
    "    performance_short = df[df['sentence_length'] < 10]\n",
    "    performance_medium = df[(df['sentence_length'] >= 10) & (df['sentence_length'] <= 20)]\n",
    "    performance_long = df[df['sentence_length'] > 20]\n",
    "    \n",
    "    # Calculate confusion matrices\n",
    "    confusion_matrix_short = confusion_matrix(performance_short['label'], performance_short['predict'])\n",
    "    confusion_matrix_medium = confusion_matrix(performance_medium['label'], performance_medium['predict'])\n",
    "    confusion_matrix_long = confusion_matrix(performance_long['label'], performance_long['predict'])\n",
    "    \n",
    "    return confusion_matrix_short, confusion_matrix_medium, confusion_matrix_long\n",
    "\n",
    "# Function to analyze review type\n",
    "def analyze_review_type(df, website):\n",
    "    \"\"\"\n",
    "    Analyzes the performance of the model based on the website type.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): DataFrame containing text data.\n",
    "    website (str): Name of the website to analyze.\n",
    "    \n",
    "    Returns:\n",
    "    array-like: Confusion matrix for the specified website.\n",
    "    \"\"\"\n",
    "    performance = df[df['website_name'] == website]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    confusion_matrix_review = confusion_matrix(performance['label'], performance['predict'])\n",
    "    \n",
    "    return confusion_matrix_review\n",
    "\n",
    "# Function to analyze sentences with negation words\n",
    "def analyze_negation_words(df):\n",
    "    \"\"\"\n",
    "    Analyzes the performance of the model based on the presence of negation words in sentences.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): DataFrame containing text data.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Tuple containing confusion matrices for sentences with and without negation words.\n",
    "    \"\"\"\n",
    "    negation_words = ['not', \"didn't\", \"shouldn't\"]\n",
    "    df['contains_negation'] = df['text'].apply(lambda x: any(word in x.split() for word in negation_words))\n",
    "    performance_with_negation = df[df['contains_negation']]\n",
    "    performance_without_negation = df[~df['contains_negation']]\n",
    "    \n",
    "    # Calculate confusion matrices\n",
    "    confusion_matrix_with_negation = confusion_matrix(performance_with_negation['label'], performance_with_negation['predict'])\n",
    "    confusion_matrix_without_negation = confusion_matrix(performance_without_negation['label'], performance_without_negation['predict'])\n",
    "    \n",
    "    return confusion_matrix_with_negation, confusion_matrix_without_negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5034f455-cec0-492e-bec8-8102c49a00c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Short Sentences:\n",
      "[[120  19]\n",
      " [ 27 120]]\n",
      "\n",
      "Confusion Matrix for Medium Sentences:\n",
      "[[108  19]\n",
      " [ 20  88]]\n",
      "\n",
      "Confusion Matrix for Long Sentences:\n",
      "[[28  6]\n",
      " [21 24]]\n",
      "\n",
      "Confusion Matrix for Amazon Reviews:\n",
      "[[89 11]\n",
      " [21 79]]\n",
      "\n",
      "Confusion Matrix for IMDb Reviews:\n",
      "[[83 17]\n",
      " [25 75]]\n",
      "\n",
      "Confusion Matrix for Sentences with Negation Words:\n",
      "[[47  1]\n",
      " [ 8  3]]\n",
      "\n",
      "Confusion Matrix for Sentences without Negation Words:\n",
      "[[209  43]\n",
      " [ 60 229]]\n"
     ]
    }
   ],
   "source": [
    "# Analyze sentence length\n",
    "short_cm, medium_cm, long_cm = analyze_sentence_length(test_analysis)\n",
    "print(\"Confusion Matrix for Short Sentences:\")\n",
    "print(short_cm)\n",
    "print(\"\\nConfusion Matrix for Medium Sentences:\")\n",
    "print(medium_cm)\n",
    "print(\"\\nConfusion Matrix for Long Sentences:\")\n",
    "print(long_cm)\n",
    "\n",
    "# Analyze review type\n",
    "amazon_cm = analyze_review_type(test_analysis, 'amazon')\n",
    "imdb_cm = analyze_review_type(test_analysis, 'imdb')\n",
    "print(\"\\nConfusion Matrix for Amazon Reviews:\")\n",
    "print(amazon_cm)\n",
    "print(\"\\nConfusion Matrix for IMDb Reviews:\")\n",
    "print(imdb_cm)\n",
    "\n",
    "# Analyze sentences with and without negation words\n",
    "with_negation_cm, without_negation_cm = analyze_negation_words(test_analysis)\n",
    "print(\"\\nConfusion Matrix for Sentences with Negation Words:\")\n",
    "print(with_negation_cm)\n",
    "print(\"\\nConfusion Matrix for Sentences without Negation Words:\")\n",
    "print(without_negation_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c9aa2",
   "metadata": {},
   "source": [
    "#### Sentence Length Analysis:\n",
    "\n",
    "- Short Sentences: The classifier demonstrates relatively balanced performance for short\n",
    "sentences, defined as those with less than 10 words. The confusion matrix reveals a slightly\n",
    "higher number of false negatives compared to false positives.\n",
    "- Medium Sentences: In medium-length sentences (10 to 20 words), the classifier exhibits\n",
    "similar behaviour, with a higher false-negative rate compared to false positives.\n",
    "- Long Sentences: For longer sentences (more than 20 words), the classifier performs\n",
    "relatively well, with a lower false-positive rate and a slightly higher false-negative rate.\n",
    "\n",
    "The classifier's performance was assessed concerning the length of input sentences. Across short,\n",
    "medium, and long sentences, the classifier exhibited varying degrees of accuracy. Notably, in short\n",
    "and medium sentences, the classifier demonstrated a tendency to misclassify negative sentiments as\n",
    "positive, as evidenced by the higher false-negative rates. Conversely, in longer sentences, the\n",
    "classifier showed improved accuracy, with a more balanced distribution of false positives and false\n",
    "negatives.\n",
    "\n",
    "#### Review Type Analysis:\n",
    "- Amazon Reviews: The classifier shows better performance on Amazon reviews, with a\n",
    "higher true positive rate compared to false positives.\n",
    "- IMDb Reviews: Its performance on IMDb reviews is slightly less accurate, with a higher\n",
    "false-negative rate and lower true positive rate.\n",
    "\n",
    "Distinct performance patterns emerged when evaluating the classifier's response to different types of\n",
    "reviews. While the classifier performed relatively well on Amazon reviews, exhibiting a higher true\n",
    "positive rate compared to false positives, its accuracy on IMDb reviews was slightly diminished,\n",
    "characterised by a higher false-negative rate. This discrepancy suggests that the classifier may\n",
    "struggle with certain types of content or sentiment expressions inherent to specific review platforms.\n",
    "\n",
    "#### Analysis of Negation Words:\n",
    "- Sentences with Negation Words: In sentences containing negation words, the classifier\n",
    "demonstrates a higher false-positive rate compared to true positives, indicating a tendency to\n",
    "misclassify negative sentiments as positive.\n",
    "- Sentences without Negation Words: Conversely, in sentences without negation words, the\n",
    "classifier shows better performance, with a higher true positive rate and lower false-positive\n",
    "rate, indicating more accurate sentiment classification.\n",
    "\n",
    "The presence of negation words within sentences posed a unique challenge to the classifier. In\n",
    "sentences containing negation words, the classifier demonstrated a propensity for misclassifying\n",
    "negative sentiments as positive, resulting in a higher false-positive rate. However, when negation\n",
    "words were absent, the classifier's performance notably improved, with a higher true positive rate and\n",
    "a lower false-positive rate, indicating more accurate sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b6be5-5750-4185-aa10-1e8dfd03ca60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
